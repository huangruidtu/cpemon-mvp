package main

import (
	"log"
	"net/http"
	"time"

	"github.com/gin-gonic/gin"
	"github.com/jmoiron/sqlx"
	"github.com/prometheus/client_golang/prometheus"
	"github.com/prometheus/client_golang/prometheus/promhttp"

	appconfig "github.com/huangruidtu/cpemon-mvp/app/pkg/config"
	appdb "github.com/huangruidtu/cpemon-mvp/app/pkg/db"
)

// ingestEventRow represents a minimal view of one row from ingest_events.
type ingestEventRow struct {
	ID       int64     `db:"id"`
	SN       string    `db:"sn"`
	EventTS  time.Time `db:"event_ts"`
	Attempts int       `db:"attempts"`
}

// ---- Prometheus metrics ----

var (
	writerRunsTotal = prometheus.NewCounter(
		prometheus.CounterOpts{
			Name: "cpemon_writer_runs_total",
			Help: "Total number of writer loop runs.",
		},
	)

	writerEventsProcessedTotal = prometheus.NewCounter(
		prometheus.CounterOpts{
			Name: "cpemon_writer_events_processed_total",
			Help: "Total number of ingest_events successfully processed by cpemon-writer.",
		},
	)

	writerEventsFailedTotal = prometheus.NewCounterVec(
		prometheus.CounterOpts{
			Name: "cpemon_writer_events_failed_total",
			Help: "Total number of ingest_events failed to process, labeled by reason.",
		},
		[]string{"reason"},
	)

	writerEventsDeadTotal = prometheus.NewCounter(
		prometheus.CounterOpts{
			Name: "cpemon_writer_events_dead_total",
			Help: "Total number of ingest_events marked as dead after too many attempts.",
		},
	)
)

// maxAttempts defines how many times we will retry a failed event
// before marking it as dead.
const maxAttempts = 5

// backoffDuration calculates exponential backoff based on attempts.
// attempts æ˜¯å½“å‰å·²ç»å°è¯•çš„æ¬¡æ•°ï¼ˆä» 0 å¼€å§‹è®¡ï¼‰ã€‚
func backoffDuration(attempts int) time.Duration {
	base := 5 * time.Second
	maxDelay := 5 * time.Minute

	// attempts: 0 -> 5s, 1 -> 10s, 2 -> 20s, 3 -> 40s, ...
	delay := base * time.Duration(1<<attempts)
	if delay > maxDelay {
		return maxDelay
	}
	return delay
}

// ç‹¬ç«‹çš„ metrics serverï¼Œç›‘å¬ :9100
func startMetricsServer() {
	mux := http.NewServeMux()
	mux.Handle("/metrics", promhttp.Handler())

	srv := &http.Server{
		Addr:    ":9100",
		Handler: mux,
	}

	go func() {
		log.Printf("[metrics] starting metrics server on %s", srv.Addr)
		if err := srv.ListenAndServe(); err != nil && err != http.ErrServerClosed {
			log.Printf("[metrics] metrics server error: %v", err)
		}
	}()
}

func main() {
	// 1. åŠ è½½é…ç½®ï¼ˆDBDSN, HTTPAddr, WorkerInterval, BatchSizeï¼‰
	cfg := appconfig.Load()

	// 2. åˆå§‹åŒ–æ•°æ®åº“è¿æ¥
	if err := appdb.Init(cfg.DBDSN); err != nil {
		log.Fatalf("failed to initialize database: %v", err)
	}

	db := appdb.Get()
	if db == nil {
		log.Fatalf("database not initialized")
	}

	// 3. æ³¨å†Œ Prometheus æŒ‡æ ‡
	prometheus.MustRegister(
		writerRunsTotal,
		writerEventsProcessedTotal,
		writerEventsFailedTotal,
		writerEventsDeadTotal,
	)

	// ğŸ‘‰ å¯åŠ¨ 9100 metrics server
	startMetricsServer()

	// 4. å¯åŠ¨åå° worker å¾ªç¯
	go func() {
		log.Printf("cpemon-writer worker loop started: interval=%s batchSize=%d", cfg.WorkerInterval, cfg.BatchSize)
		ticker := time.NewTicker(cfg.WorkerInterval)
		defer ticker.Stop()

		for range ticker.C {
			writerRunsTotal.Inc()
			if err := runOnce(cfg.BatchSize); err != nil {
				log.Printf("cpemon-writer runOnce error: %v", err)
			}
		}
	}()

	// 5. æš´éœ² /healthz å’Œ /metrics
	r := gin.Default()

	// /healthz: ç®€å•è¿”å› ok
	r.GET("/healthz", func(c *gin.Context) {
		c.JSON(http.StatusOK, gin.H{
			"status":  "ok",
			"service": "cpemon-writer",
		})
	})

	// /metrics: æš´éœ² Prometheus æŒ‡æ ‡
	r.GET("/metrics", gin.WrapH(promhttp.Handler()))

	log.Printf("cpemon-writer listening on %s\n", cfg.HTTPAddr)
	if err := r.Run(cfg.HTTPAddr); err != nil {
		log.Fatalf("failed to start cpemon-writer: %v", err)
	}
}

// runOnce processes up to batchSize queued events in ingest_events.
// ç®€åŒ–ç‰ˆé€»è¾‘ï¼š
// 1. æ‰¾åˆ° status='queued' & next_at <= NOW() çš„äº‹ä»¶ï¼ˆLIMIT batchSizeï¼‰ã€‚
// 2. å°è¯•â€œé”å®šâ€è¿™æ¡äº‹ä»¶ï¼šUPDATE status='processing', attempts=attempts+1 WHERE id=? AND status='queued'ã€‚
//    - å¦‚æœè¡Œæ•°=0ï¼Œè¯´æ˜è¢«åˆ«äººæŠ¢åˆ°äº†ï¼Œè·³è¿‡ã€‚
// 3. å¤„ç†äº‹ä»¶ï¼ˆå½“å‰ç‰ˆæœ¬åªæ˜¯æ‰“å°æ—¥å¿—ï¼Œä¸æ›´æ–° cpe_status / historyï¼‰ã€‚
// 4. æˆåŠŸï¼šUPDATE ingest_events SET status='done', updated_at=NOW() WHERE id=?ã€‚
func runOnce(batchSize int) error {
	db := appdb.Get()
	if db == nil {
		return nil
	}

	// 1. æŸ¥è¯¢å¾…å¤„ç†äº‹ä»¶
	var events []ingestEventRow
	query := `
SELECT
  id, sn, event_ts, attempts
FROM ingest_events
WHERE status = 'queued'
  AND next_at <= NOW()
ORDER BY id
LIMIT ?
`
	if err := db.Select(&events, query, batchSize); err != nil {
		return err
	}

	if len(events) == 0 {
		return nil
	}

	for _, ev := range events {
		if err := processOneEvent(db, &ev); err != nil {
			// è¿™é‡Œåªè®°å½•æ—¥å¿—ï¼Œå…·ä½“é”™è¯¯åˆ†ç±»å’Œ backoff åœ¨ processOneEvent é‡Œå¤„ç†
			log.Printf("processOneEvent error for id=%d sn=%s: %v", ev.ID, ev.SN, err)
		}
	}

	return nil
}

// processOneEvent tries to "lock" one event and process it.
//
// è¿”å› error ä»…ç”¨äºæ—¥å¿—è®°å½•ï¼ŒçœŸæ­£çš„é‡è¯•/æ ‡è®° dead é€»è¾‘éƒ½åœ¨é‡Œé¢å®Œæˆã€‚
func processOneEvent(db *sqlx.DB, ev *ingestEventRow) error {
	// 1. å°è¯•é”å®šè¿™æ¡è®°å½•ï¼šä» queued -> processingï¼Œattempts+1
	res, err := db.Exec(`
UPDATE ingest_events
SET status = 'processing',
    attempts = attempts + 1,
    updated_at = NOW()
WHERE id = ?
  AND status = 'queued'
`, ev.ID)
	if err != nil {
		writerEventsFailedTotal.WithLabelValues("lock_error").Inc()
		return err
	}

	rows, err := res.RowsAffected()
	if err != nil {
		writerEventsFailedTotal.WithLabelValues("rows_affected_error").Inc()
		return err
	}
	if rows == 0 {
		// æ²¡æŠ¢åˆ°é”ï¼Œè¯´æ˜è¢«åˆ«çš„ writer å¤„ç†äº†ï¼Œç›´æ¥è·³è¿‡å³å¯ã€‚
		return nil
	}

	// 2. æ¨¡æ‹Ÿå¤„ç†é€»è¾‘
	// ç°åœ¨æˆ‘ä»¬è¿˜æ²¡æœ‰çœŸæ­£çš„ä¸šåŠ¡å¤„ç†ï¼ˆæ¯”å¦‚å†™å…¥ cpe_status/historyï¼‰ï¼Œ
	// ä¸ºäº†è®©é˜Ÿåˆ—èƒ½è·‘é€šï¼Œè¿™é‡Œå…ˆç®€å•æ‰“å°ä¸€æ¡æ—¥å¿—ï¼Œå¹¶è®¤ä¸ºå¤„ç†æˆåŠŸã€‚
	log.Printf("processing ingest_event id=%d sn=%s event_ts=%s",
		ev.ID, ev.SN, ev.EventTS.Format(time.RFC3339Nano))

	// 3. å½“å‰ç‰ˆæœ¬ï¼šå¤„ç†æˆåŠŸï¼Œç›´æ¥æ ‡è®°ä¸º done
	if err := markDone(db, ev.ID); err != nil {
		writerEventsFailedTotal.WithLabelValues("mark_done_error").Inc()
		return err
	}

	writerEventsProcessedTotal.Inc()
	return nil
}

// markDone sets status='done' for event id.
func markDone(db *sqlx.DB, id int64) error {
	_, err := db.Exec(`
UPDATE ingest_events
SET status = 'done',
    updated_at = NOW()
WHERE id = ?
`, id)
	return err
}

// handleFailure updates attempts/next_at/status based on the newAttempts.
// ç›®å‰æˆ‘ä»¬è¿˜æ²¡åœ¨æµç¨‹é‡ŒçœŸæ­£è°ƒç”¨å®ƒï¼Œé¢„ç•™ç»™ä»¥åâ€œä¸šåŠ¡å¤„ç†å‡ºé”™æ—¶çš„é‡è¯•/æ­»ä¿¡â€åœºæ™¯ã€‚
func handleFailure(db *sqlx.DB, ev *ingestEventRow, newAttempts int, reason string) error {
	now := time.Now()

	if newAttempts >= maxAttempts {
		// Too many failures: mark as dead.
		_, err := db.Exec(`
UPDATE ingest_events
SET status = 'dead',
    attempts = ?,
    updated_at = ?
WHERE id = ?
`, newAttempts, now, ev.ID)
		if err == nil {
			writerEventsDeadTotal.Inc()
		}
		writerEventsFailedTotal.WithLabelValues(reason).Inc()
		return err
	}

	// Otherwise, schedule a retry with backoff.
	delay := backoffDuration(newAttempts)
	nextAt := now.Add(delay)

	_, err := db.Exec(`
UPDATE ingest_events
SET status = 'queued',
    attempts = ?,
    next_at = ?,
    updated_at = ?
WHERE id = ?
`, newAttempts, nextAt, now, ev.ID)
	if err != nil {
		writerEventsFailedTotal.WithLabelValues("update_retry_state_error").Inc()
	} else {
		writerEventsFailedTotal.WithLabelValues(reason).Inc()
	}
	return err
}
